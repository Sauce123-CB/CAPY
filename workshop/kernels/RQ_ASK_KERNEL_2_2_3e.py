"""
RQ_ASK_KERNEL_2_2_3e.py
CAPY Pipeline - Research Question Execution via Parallel Subagents

This kernel executes the 7 Research Questions generated by RQ_GEN using
parallel subagents with web search capabilities (Claude Opus or Gemini Deep Research).

Architecture (v2.2.3):
- 7 parallel RQ slots (4 mandatory + 3 dynamic)
- M-1: Integrity Check
- M-2: Adversarial Synthesis
- M-3a: Mainline Scenario H.A.D. (4 scenarios)
- M-3b: Tail Scenario H.A.D. (4 scenarios: 2 Blue Sky + 2 Black Swan)
- D-1, D-2, D-3: Lynchpin/Dynamic allocation

Execution Modes:
- "claude": Claude Opus subagents via Task tool (recommended)
- "gemini_cli": Gemini Deep Research via CLI
- "gemini_sdk": Gemini via Python SDK

Usage:
    from RQ_ASK_KERNEL_2_2_3e import execute_research_plan

    results = await execute_research_plan(
        research_plan=a8_research_strategy_map,
        ticker="DAVE",
        max_concurrent=7,
        executor="claude"
    )
"""

import asyncio
import json
import os
import subprocess
import tempfile
import logging
from datetime import datetime
from pathlib import Path
from typing import Optional
from dataclasses import dataclass, field, asdict

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger("RQ_ASK")

# ============================================================================
# CONFIGURATION
# ============================================================================

# Deep Research model - updated Dec 2025
# Using gemini-2.5-pro which supports web search grounding via CLI
# With Gemini 3 Pro available through Ultra subscription
DEEP_RESEARCH_MODEL = "gemini-2.5-pro"

# Deep Research Agent wrapper - turns Gemini into an iterative research agent
# This harness instructs the model to plan, search, iterate, reason, and report
DEEP_RESEARCH_AGENT_WRAPPER = """You are a Deep Research Agent focused on financial analysis.
Your goal is to fully answer the research question below.

METHODOLOGY:
1. PLAN: Outline what data and sources you need to answer this question
2. SEARCH: Use your search tool to find relevant data from SEC filings, earnings calls, analyst reports, and financial news
3. READ: Analyze the search results carefully
4. ITERATE: If data is missing or incomplete, search again with refined queries
5. REASON: Apply financial and business logic to synthesize your findings
6. REPORT: Provide a detailed answer with specific citations

REQUIREMENTS:
- Prefer primary sources: SEC filings (10-K, 10-Q, 8-K, proxy statements), earnings call transcripts, investor presentations
- Include specific numbers, dates, and quotes where available
- If you cannot find authoritative data on a point, say so explicitly
- Structure your response clearly with sections for each key finding

RESEARCH QUESTION:
{query}"""

# Gemini CLI path (set by npm global install)
GEMINI_CLI_PATH = os.path.expanduser("~/.npm-global/bin/gemini")

# Max concurrent queries (7-slot architecture in v2.2.3)
MAX_CONCURRENT = 7

# Query timeout (Deep Research can take 2-5 minutes per query)
QUERY_TIMEOUT_SECONDS = 600

# Claude Opus Deep Research wrapper - enhanced citation requirements
CLAUDE_DEEP_RESEARCH_WRAPPER = """You are a Deep Research Agent specialized in financial analysis and equity research.
Your goal is to thoroughly answer the research question below using web search.

METHODOLOGY:
1. PLAN: Identify the specific data points, sources, and search queries needed
2. SEARCH: Execute multiple targeted web searches to gather comprehensive data
3. ANALYZE: Critically evaluate search results for relevance and reliability
4. ITERATE: If key data is missing, refine your search strategy and search again
5. SYNTHESIZE: Apply financial reasoning to connect findings and draw conclusions
6. REPORT: Deliver a detailed, well-structured answer

SOURCE PRIORITY (in order of preference):
1. SEC filings (10-K, 10-Q, 8-K, DEF 14A proxy statements) via EDGAR
2. Earnings call transcripts and investor presentations
3. Company press releases
4. Reputable financial news (WSJ, Bloomberg, Reuters, FT)
5. Analyst reports and research notes
6. Industry publications and trade journals

CITATION REQUIREMENTS:
- Every factual claim MUST include an inline citation
- Use this format: [Source Name, Date] or [SEC Filing Type, Filing Date]
- Examples:
  - "Revenue grew 42% YoY to $150.8M [Q3 2025 10-Q, Nov 2025]"
  - "The DOJ lawsuit alleges deceptive practices [WSJ, Dec 30 2024]"
  - "Management guided to 25% EBITDA margins [Q3 2025 Earnings Call, Nov 2025]"
- At the end of your response, include a SOURCES section listing all referenced materials

OUTPUT STRUCTURE:
## Executive Summary
[2-3 sentence overview of key findings]

## Detailed Findings
[Organized by the sub-questions in the research question, with citations]

## Key Uncertainties
[What you could NOT find or verify, and why]

## Sources
[Numbered list of all sources cited, with URLs where available]

---

RESEARCH QUESTION:
{query}"""

# ============================================================================
# DATA STRUCTURES
# ============================================================================

@dataclass
class ResearchQuery:
    """Single research query from A.8 Research Plan."""
    rq_id: str
    allocation_type: str  # MANDATORY | DYNAMIC
    coverage_objective: str  # M-1/M-2/M-3 or Lynchpin ID
    platform: str  # GDR (all queries now route to GDR)
    platform_rationale: str
    a7_linkage: str
    prompt_text: str


@dataclass
class ResearchResult:
    """Result from a single Deep Research query."""
    rq_id: str
    status: str  # SUCCESS | ERROR | TIMEOUT
    query_text: str
    response_text: str
    sources: list = field(default_factory=list)
    execution_time_seconds: float = 0.0
    timestamp: str = ""
    error_message: Optional[str] = None

    def to_dict(self):
        return asdict(self)


@dataclass
class ResearchPlanResults:
    """Complete results from executing the research plan."""
    ticker: str
    execution_timestamp: str
    total_queries: int
    successful_queries: int
    failed_queries: int
    results: list
    execution_time_seconds: float

    def to_dict(self):
        d = asdict(self)
        d['results'] = [r.to_dict() if hasattr(r, 'to_dict') else r for r in self.results]
        return d


# ============================================================================
# GEMINI DEEP RESEARCH EXECUTION
# ============================================================================

async def execute_deep_research_cli(query: ResearchQuery) -> ResearchResult:
    """
    Execute a Deep Research query using Gemini CLI.

    The CLI handles OAuth authentication automatically using cached credentials.
    """
    start_time = datetime.now()
    logger.info(f"[{query.rq_id}] Starting Deep Research query...")

    try:
        # Create temp file for the query
        with tempfile.NamedTemporaryFile(mode='w', suffix='.txt', delete=False) as f:
            f.write(query.prompt_text)
            query_file = f.name

        # Wrap the query with the Deep Research Agent harness
        wrapped_prompt = DEEP_RESEARCH_AGENT_WRAPPER.format(query=query.prompt_text)

        # Build command
        cmd = [
            GEMINI_CLI_PATH,
            "-m", DEEP_RESEARCH_MODEL,
            "-p", wrapped_prompt
        ]

        # Execute with timeout
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=QUERY_TIMEOUT_SECONDS
            )
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()
            os.unlink(query_file)

            return ResearchResult(
                rq_id=query.rq_id,
                status="TIMEOUT",
                query_text=query.prompt_text,
                response_text="",
                execution_time_seconds=(datetime.now() - start_time).total_seconds(),
                timestamp=datetime.now().isoformat(),
                error_message=f"Query timed out after {QUERY_TIMEOUT_SECONDS}s"
            )

        os.unlink(query_file)

        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "Unknown error"
            logger.error(f"[{query.rq_id}] CLI error: {error_msg}")

            return ResearchResult(
                rq_id=query.rq_id,
                status="ERROR",
                query_text=query.prompt_text,
                response_text="",
                execution_time_seconds=(datetime.now() - start_time).total_seconds(),
                timestamp=datetime.now().isoformat(),
                error_message=error_msg
            )

        response_text = stdout.decode()
        execution_time = (datetime.now() - start_time).total_seconds()

        logger.info(f"[{query.rq_id}] Completed in {execution_time:.1f}s")

        return ResearchResult(
            rq_id=query.rq_id,
            status="SUCCESS",
            query_text=query.prompt_text,
            response_text=response_text,
            sources=[],  # TODO: Parse sources from response if available
            execution_time_seconds=execution_time,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.exception(f"[{query.rq_id}] Unexpected error")
        return ResearchResult(
            rq_id=query.rq_id,
            status="ERROR",
            query_text=query.prompt_text,
            response_text="",
            execution_time_seconds=(datetime.now() - start_time).total_seconds(),
            timestamp=datetime.now().isoformat(),
            error_message=str(e)
        )


async def execute_deep_research_sdk(query: ResearchQuery) -> ResearchResult:
    """
    Execute a Deep Research query using google-genai SDK.

    Requires GEMINI_API_KEY environment variable.
    Note: SDK v1.47.0 may not have Interactions API for true Deep Research.
    Falls back to standard generation.
    """
    start_time = datetime.now()
    logger.info(f"[{query.rq_id}] Starting SDK query...")

    try:
        from google import genai

        api_key = os.environ.get("GEMINI_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY not found in environment")

        client = genai.Client(api_key=api_key)

        # Standard generation (not Deep Research)
        response = client.models.generate_content(
            model="gemini-2.0-flash-exp",
            contents=query.prompt_text
        )

        execution_time = (datetime.now() - start_time).total_seconds()
        logger.info(f"[{query.rq_id}] Completed in {execution_time:.1f}s")

        return ResearchResult(
            rq_id=query.rq_id,
            status="SUCCESS",
            query_text=query.prompt_text,
            response_text=response.text,
            sources=[],
            execution_time_seconds=execution_time,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.exception(f"[{query.rq_id}] SDK error")
        return ResearchResult(
            rq_id=query.rq_id,
            status="ERROR",
            query_text=query.prompt_text,
            response_text="",
            execution_time_seconds=(datetime.now() - start_time).total_seconds(),
            timestamp=datetime.now().isoformat(),
            error_message=str(e)
        )


# ============================================================================
# CLAUDE OPUS EXECUTION
# ============================================================================

async def execute_claude_opus(query: ResearchQuery) -> ResearchResult:
    """
    Execute a Deep Research query using Claude Opus via Claude Code CLI.

    Uses the claude CLI with WebSearch tool access for research.
    """
    start_time = datetime.now()
    logger.info(f"[{query.rq_id}] Starting Claude Opus research query...")

    try:
        # Wrap the query with the Claude Deep Research harness
        wrapped_prompt = CLAUDE_DEEP_RESEARCH_WRAPPER.format(query=query.prompt_text)

        # Build command - use claude CLI in print mode
        cmd = [
            "claude",
            "-p", wrapped_prompt,
            "--output-format", "text"
        ]

        # Execute with timeout
        process = await asyncio.create_subprocess_exec(
            *cmd,
            stdout=asyncio.subprocess.PIPE,
            stderr=asyncio.subprocess.PIPE
        )

        try:
            stdout, stderr = await asyncio.wait_for(
                process.communicate(),
                timeout=QUERY_TIMEOUT_SECONDS
            )
        except asyncio.TimeoutError:
            process.kill()
            await process.wait()

            return ResearchResult(
                rq_id=query.rq_id,
                status="TIMEOUT",
                query_text=query.prompt_text,
                response_text="",
                execution_time_seconds=(datetime.now() - start_time).total_seconds(),
                timestamp=datetime.now().isoformat(),
                error_message=f"Query timed out after {QUERY_TIMEOUT_SECONDS}s"
            )

        if process.returncode != 0:
            error_msg = stderr.decode() if stderr else "Unknown error"
            logger.error(f"[{query.rq_id}] Claude CLI error: {error_msg}")

            return ResearchResult(
                rq_id=query.rq_id,
                status="ERROR",
                query_text=query.prompt_text,
                response_text="",
                execution_time_seconds=(datetime.now() - start_time).total_seconds(),
                timestamp=datetime.now().isoformat(),
                error_message=error_msg
            )

        response_text = stdout.decode()
        execution_time = (datetime.now() - start_time).total_seconds()

        logger.info(f"[{query.rq_id}] Completed in {execution_time:.1f}s")

        return ResearchResult(
            rq_id=query.rq_id,
            status="SUCCESS",
            query_text=query.prompt_text,
            response_text=response_text,
            sources=[],  # Could parse from SOURCES section
            execution_time_seconds=execution_time,
            timestamp=datetime.now().isoformat()
        )

    except Exception as e:
        logger.exception(f"[{query.rq_id}] Unexpected error")
        return ResearchResult(
            rq_id=query.rq_id,
            status="ERROR",
            query_text=query.prompt_text,
            response_text="",
            execution_time_seconds=(datetime.now() - start_time).total_seconds(),
            timestamp=datetime.now().isoformat(),
            error_message=str(e)
        )


# ============================================================================
# MAIN EXECUTION
# ============================================================================

async def execute_research_plan(
    research_plan: dict,
    ticker: str,
    max_concurrent: int = MAX_CONCURRENT,
    executor: str = "gemini_cli"
) -> ResearchPlanResults:
    """
    Execute the full research plan from A.8_RESEARCH_STRATEGY_MAP.

    Args:
        research_plan: The A.8 artifact from RQ_GEN
        ticker: Company ticker symbol
        max_concurrent: Maximum concurrent queries (default 6)
        executor: Which executor to use:
            - "gemini_cli": Gemini via CLI (default)
            - "gemini_sdk": Gemini via SDK
            - "claude": Claude Opus via Claude Code CLI

    Returns:
        ResearchPlanResults with all query results
    """
    start_time = datetime.now()

    # Parse research plan
    rq_list = research_plan.get("Research_Plan", [])
    if not rq_list:
        raise ValueError("Research_Plan is empty")

    queries = [
        ResearchQuery(
            rq_id=rq["RQ_ID"],
            allocation_type=rq.get("Allocation_Type", "DYNAMIC"),
            coverage_objective=rq.get("Coverage_Objective", ""),
            platform=rq.get("Platform", "GDR"),
            platform_rationale=rq.get("Platform_Rationale", ""),
            a7_linkage=rq.get("A7_Linkage", ""),
            prompt_text=rq["Prompt_Text"]
        )
        for rq in rq_list
    ]

    logger.info(f"Executing {len(queries)} research queries for {ticker}")
    logger.info(f"Max concurrent: {max_concurrent}, Executor: {executor}")

    # Select execution function
    if executor == "claude":
        execute_fn = execute_claude_opus
    elif executor == "gemini_sdk":
        execute_fn = execute_deep_research_sdk
    else:
        execute_fn = execute_deep_research_cli

    # Execute with semaphore for concurrency control
    semaphore = asyncio.Semaphore(max_concurrent)

    async def execute_with_semaphore(query: ResearchQuery) -> ResearchResult:
        async with semaphore:
            return await execute_fn(query)

    # Run all queries concurrently (up to max_concurrent at a time)
    results = await asyncio.gather(
        *[execute_with_semaphore(q) for q in queries],
        return_exceptions=True
    )

    # Convert exceptions to error results
    processed_results = []
    for i, result in enumerate(results):
        if isinstance(result, Exception):
            processed_results.append(ResearchResult(
                rq_id=queries[i].rq_id,
                status="ERROR",
                query_text=queries[i].prompt_text,
                response_text="",
                error_message=str(result)
            ))
        else:
            processed_results.append(result)

    # Compute stats
    successful = sum(1 for r in processed_results if r.status == "SUCCESS")
    failed = len(processed_results) - successful
    total_time = (datetime.now() - start_time).total_seconds()

    logger.info(f"Completed: {successful}/{len(processed_results)} successful in {total_time:.1f}s")

    return ResearchPlanResults(
        ticker=ticker,
        execution_timestamp=datetime.now().isoformat(),
        total_queries=len(processed_results),
        successful_queries=successful,
        failed_queries=failed,
        results=processed_results,
        execution_time_seconds=total_time
    )


def save_results(results: ResearchPlanResults, output_dir: str) -> str:
    """
    Save research results to JSON file.

    Args:
        results: The ResearchPlanResults object
        output_dir: Directory to save results

    Returns:
        Path to saved file
    """
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    filename = f"A9_RESEARCH_RESULTS_{results.ticker}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
    filepath = output_path / filename

    with open(filepath, 'w') as f:
        json.dump(results.to_dict(), f, indent=2)

    logger.info(f"Results saved to {filepath}")
    return str(filepath)


# ============================================================================
# STANDALONE EXECUTION
# ============================================================================

async def main():
    """Test execution with sample research plan."""

    # Sample A.8 for testing
    sample_plan = {
        "A.8_RESEARCH_STRATEGY_MAP": {
            "schema_version": "G3_2.2.2",
            "Research_Plan": [
                {
                    "RQ_ID": "RQ1",
                    "Allocation_Type": "MANDATORY",
                    "Coverage_Objective": "M-1",
                    "Platform": "GDR",
                    "Platform_Rationale": "Test query",
                    "A7_Linkage": "Test",
                    "Prompt_Text": "What are the key financial metrics for Dave Inc (NASDAQ:DAVE) as of December 2024?"
                }
            ]
        }
    }

    results = await execute_research_plan(
        research_plan=sample_plan["A.8_RESEARCH_STRATEGY_MAP"],
        ticker="DAVE",
        max_concurrent=1,
        use_cli=True
    )

    print(json.dumps(results.to_dict(), indent=2))


if __name__ == "__main__":
    asyncio.run(main())
